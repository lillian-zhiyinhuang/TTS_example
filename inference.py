# ==============================================================================
# æª”æ¡ˆï¼šinference.py
# æè¿°ï¼šä¸€ä»½å°ˆç‚ºé«˜æ•ˆèƒ½ TTS (æ–‡å­—è½‰èªéŸ³) è¨­è¨ˆçš„æ¨ç†å¼•æ“ã€‚
#        æ­¤è…³æœ¬å°‡æ¨¡å‹å°è£æ–¼ä¸€å€‹é¡åˆ¥ä¸­ï¼Œä¸¦æ‡‰ç”¨å¤šé …å„ªåŒ–æŠ€è¡“ï¼Œä»¥å¯¦ç¾å¿«é€Ÿã€
#        ä½å»¶é²çš„èªéŸ³ç”Ÿæˆï¼Œç‰¹åˆ¥é©åˆæ‰¹æ¬¡è™•ç†ä»»å‹™ã€‚
#
# æ ¸å¿ƒç­–ç•¥ï¼š
# 1. æ¥µè‡´æ€§èƒ½å„ªåŒ– (Performance Optimization)ï¼š
#    - 4-bit é‡åŒ–è¼‰å…¥ï¼šä½¿ç”¨ `BitsAndBytes` ä»¥ NF4 æ ¼å¼è¼‰å…¥æ¨¡å‹ï¼Œå¤§å¹…é™ä½ VRAM
#      ä½”ç”¨ï¼Œä½¿å¾—åœ¨æ¶ˆè²»ç´šç¡¬é«”ä¸Šé‹è¡Œå¤§å‹æ¨¡å‹æˆç‚ºå¯èƒ½ã€‚
#    - JIT å³æ™‚ç·¨è­¯ (torch.compile)ï¼šåˆ©ç”¨ PyTorch 2.0+ çš„ `torch.compile`
#      åŠŸèƒ½ï¼Œå°‡æ¨¡å‹è¨ˆç®—åœ–è½‰æ›ç‚ºå„ªåŒ–çš„åº•å±¤æ ¸å¿ƒï¼Œé¡¯è‘—æ¸›å°‘ Python è§£é‡‹å™¨çš„é–‹éŠ·ï¼Œ
#      æå‡ç”Ÿæˆé€Ÿåº¦ã€‚
# 2. ç‰©ä»¶å°å‘å°è£ (Object-Oriented Encapsulation)ï¼š
#    - å°‡æ‰€æœ‰æ¨¡å‹ (LLMã€Tokenizerã€SNAC) å’Œç›¸é—œé‚è¼¯å°è£åœ¨ `OptimizedOrpheusTTS`
#      é¡åˆ¥ä¸­ã€‚é€™ç¢ºä¿äº†æ¨¡å‹åƒ…åœ¨åˆå§‹åŒ–æ™‚è¼‰å…¥ä¸€æ¬¡ï¼Œé¿å…äº†åœ¨è™•ç†å¤šå€‹è«‹æ±‚æ™‚çš„
#      é‡è¤‡è¼‰å…¥é–‹éŠ·ã€‚
# 3. æ‰¹æ¬¡æ¨ç†æµç¨‹ (Batch Processing Workflow)ï¼š
#    - ä¸»ç¨‹å¼å€å¡Šæ¡ç”¨éäº’å‹•å¼çš„æ‰¹æ¬¡è™•ç†æ¨¡å¼ï¼Œä¸€æ¬¡æ€§è®€å–æ‰€æœ‰å¾…ç”Ÿæˆçš„å¥å­ï¼Œ
#      ä¸¦åœ¨å–®ä¸€å¼•æ“å¯¦ä¾‹ä¸Šå¾ªç’°è™•ç†ï¼Œæœ€å¤§åŒ–ç¡¬é«”åˆ©ç”¨ç‡ä¸¦æ”¤éŠ·æ¨¡å‹è¼‰å…¥æˆæœ¬ã€‚
# 4. ç²¾æº–çš„éŸ³è¨Šå¾Œè™•ç† (Precise Audio Post-processing)ï¼š
#    - åŒ…å«å¾ç”Ÿæˆ Tokens ä¸­ç²¾ç¢ºæå–éŸ³è¨Šç¢¼æµçš„é‚è¼¯ï¼Œä»¥åŠä½¿ç”¨ `pyrubberband`
#      é€²è¡ŒéŸ³é«˜ä¸è®Šçš„èªé€Ÿèª¿æ•´ã€‚
# ==============================================================================

import torch
import numpy as np
import soundfile as sf
import pyrubberband as pyrb
from pathlib import Path
from typing import List, Tuple
from opencc import OpenCC
from snac import SNAC
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

class OptimizedOrpheusTTS:
    """
    ç¶“éæ€§èƒ½å„ªåŒ–çš„ TTS ç”Ÿæˆå™¨ï¼Œæ¡ç”¨ 4-bit é‡åŒ–å’Œ torch.compile åŠ é€Ÿã€‚
    
    Attributes:
        model (torch.nn.Module): ç¶“éé‡åŒ–èˆ‡ç·¨è­¯çš„å› æœèªè¨€æ¨¡å‹ã€‚
        tokenizer (AutoTokenizer): æ–‡æœ¬åˆ†è©å™¨ã€‚
        snac_model (SNAC): ç”¨æ–¼å°‡éŸ³è¨Š Tokens è§£ç¢¼å›æ³¢å½¢çš„ SNAC æ¨¡å‹ã€‚
        cc (OpenCC): ç”¨æ–¼ç°¡ç¹è½‰æ›çš„å·¥å…·ã€‚
    """
    def __init__(self, model_id: str, tokenizer_id: str, snac_id: str = "hubertsiuzdak/snac_24khz"):
        """
        åˆå§‹åŒ– TTS å¼•æ“ï¼ŒåŒ…æ‹¬è¼‰å…¥ã€é‡åŒ–å’Œç·¨è­¯æ¨¡å‹ã€‚
        
        Args:
            model_id (str): Hugging Face Hub ä¸Šçš„æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾‘ã€‚
            tokenizer_id (str): Hugging Face Hub ä¸Šçš„åˆ†è©å™¨ ID æˆ–æœ¬åœ°è·¯å¾‘ã€‚
            snac_id (str): Hugging Face Hub ä¸Šçš„ SNAC æ¨¡å‹ IDã€‚
        """
        print(">>> æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹èˆ‡ Tokenizer (å·²å•Ÿç”¨æ€§èƒ½å„ªåŒ–)...")
        
        # --- 1. è¨­å®š 4-bit é‡åŒ–çµ„æ…‹ ---
        compute_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16
        
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=compute_dtype,
            bnb_4bit_use_double_quant=True,
        )
        
        print(f">>> ä½¿ç”¨ 4-bit é‡åŒ–è¼‰å…¥ï¼Œè¨ˆç®—ç²¾åº¦ç‚º: {compute_dtype}")
        
        # --- 2. è¼‰å…¥é‡åŒ–å¾Œçš„æ¨¡å‹ ---
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            quantization_config=quantization_config,
            device_map="auto" # è‡ªå‹•å°‡æ¨¡å‹åˆ†é…åˆ°å¯ç”¨ç¡¬é«”
        )
        
        # --- 3. JIT å³æ™‚ç·¨è­¯æ¨¡å‹ ---
        # `torch.compile` æ˜¯ PyTorch 2.0 çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œå®ƒå°‡ Python ç¨‹å¼ç¢¼è½‰æ›ç‚º
        # é«˜æ•ˆçš„åº•å±¤ç¨‹å¼ç¢¼ï¼Œèƒ½å¤§å¹…æå‡é‹ç®—é€Ÿåº¦ã€‚
        # - mode="reduce-overhead": æ¸›å°‘æ¯æ¬¡æ“ä½œä¹‹é–“çš„ Python æ¡†æ¶é–‹éŠ·ï¼Œå°å°æ‰¹é‡ç‰¹åˆ¥æœ‰æ•ˆã€‚
        # - fullgraph=True: å˜—è©¦å°‡æ•´å€‹æ¨¡å‹ç·¨è­¯æˆä¸€å€‹å–®ä¸€çš„è¨ˆç®—åœ–ï¼Œå¦‚æœæˆåŠŸï¼Œèƒ½ç²å¾—æœ€å¤§åŠ é€Ÿã€‚
        print(">>> æ­£åœ¨ä½¿ç”¨ torch.compile() ç·¨è­¯æ¨¡å‹ä»¥ç²å–æ¥µè‡´æ€§èƒ½...")
        try:
            self.model = torch.compile(self.model, mode="reduce-overhead", fullgraph=True)
            print("âœ… æ¨¡å‹ç·¨è­¯æˆåŠŸã€‚")
        except Exception as e:
            # ç·¨è­¯å¯èƒ½å› ç¡¬é«”ä¸æ”¯æ´æˆ–æ¨¡å‹æ¶æ§‹è¤‡é›œè€Œå¤±æ•—ï¼Œæ­¤è™•ä½œå›é€€è™•ç†ã€‚
            print(f"âš ï¸ æ¨¡å‹ç·¨è­¯å¤±æ•—ï¼Œå°‡ä½¿ç”¨æœªç·¨è­¯ç‰ˆæœ¬ (é€Ÿåº¦è¼ƒæ…¢)ã€‚éŒ¯èª¤: {e}")

        # --- 4. è¼‰å…¥å…¶é¤˜è¼”åŠ©æ¨¡å‹ ---
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)
        # SNAC è§£ç¢¼åœ¨ CPU ä¸Šé€²è¡Œå³å¯ï¼Œå› ç‚ºå®ƒåœ¨ LLM ç”Ÿæˆä¹‹å¾ŒåŸ·è¡Œï¼Œè¨ˆç®—é‡ä¸å¤§ã€‚
        self.snac_model = SNAC.from_pretrained(snac_id).cpu()
        self.cc = OpenCC('t2s.json')
        
        print("âœ… æ‰€æœ‰æ¨¡å‹å‡å·²æˆåŠŸè¼‰å…¥ä¸¦å„ªåŒ–ã€‚")

    def _prepare_prompts_for_batch(self, prompts: List[str], voice: str) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        æº–å‚™æ‰¹æ¬¡åŒ–çš„æç¤ºè©ï¼ŒåŒ…æ‹¬ç°¡ç¹è½‰æ›ã€æ ¼å¼åŒ–å’Œå·¦å¡«å……ã€‚
        
        ğŸ¯ å·¦å¡«å…… (Left Padding) çš„é‡è¦æ€§ï¼š
        å°æ–¼è‡ªå›æ­¸æ¨¡å‹ (Autoregressive Models) çš„æ‰¹æ¬¡ç”Ÿæˆï¼Œå¿…é ˆä½¿ç”¨å·¦å¡«å……ã€‚
        é€™ç¢ºä¿äº†æ¯å€‹åºåˆ—çš„çµå°¾ (å³æ¨¡å‹é–‹å§‹ç”Ÿæˆæ–° Token çš„åœ°æ–¹) éƒ½æ˜¯å°é½Šçš„ï¼Œ
        è®“ GPU å¯ä»¥é«˜æ•ˆåœ°ä¸¦è¡Œè™•ç†æ‰€æœ‰åºåˆ—çš„ä¸‹ä¸€æ­¥é æ¸¬ã€‚
        """
        simplified_prompts = [self.cc.convert(p) for p in prompts]
        formatted_prompts = [f"{voice}: {p}" for p in simplified_prompts]
        
        # å®šç¾©æç¤ºè©æ¨¡æ¿æ‰€éœ€çš„ç‰¹æ®Š Tokens
        start_token = torch.tensor([[128259]], dtype=torch.long) # <|start_of_human|>
        end_tokens = torch.tensor([[128009, 128260]], dtype=torch.long) # <|end_of_text|><|end_of_human|>
        
        # å°‡æ‰€æœ‰æç¤ºè©è½‰æ›ç‚ºåŒ…å«ç‰¹æ®Š Tokens çš„å¼µé‡
        all_modified_input_ids = []
        for p in formatted_prompts:
            input_ids = self.tokenizer(p, return_tensors="pt").input_ids
            modified_ids = torch.cat([start_token, input_ids, end_tokens], dim=1)
            all_modified_input_ids.append(modified_ids)
            
        # è¨ˆç®—æ‰¹æ¬¡ä¸­æœ€é•·åºåˆ—çš„é•·åº¦ï¼Œä¸¦é€²è¡Œå·¦å¡«å……
        max_length = max(ids.shape[1] for ids in all_modified_input_ids)
        all_padded_tensors, all_attention_masks = [], []
        padding_token_id = 128263 # å¿…é ˆèˆ‡è¨“ç·´æ™‚ä½¿ç”¨çš„ pad_token_id ä¸€è‡´
        
        for ids in all_modified_input_ids:
            padding_size = max_length - ids.shape[1]
            # åœ¨å·¦å´å¡«å…… padding_token_id
            padded_tensor = torch.cat([torch.full((1, padding_size), padding_token_id, dtype=torch.long), ids], dim=1)
            # Attention mask åŒæ¨£åœ¨å·¦å´è£œ 0
            attention_mask = torch.cat([torch.zeros((1, padding_size), dtype=torch.long), torch.ones((1, ids.shape[1]), dtype=torch.long)], dim=1)
            all_padded_tensors.append(padded_tensor)
            all_attention_masks.append(attention_mask)
            
        # å°‡å–®å€‹å¼µé‡åˆä½µæˆä¸€å€‹æ‰¹æ¬¡
        return torch.cat(all_padded_tensors, dim=0), torch.cat(all_attention_masks, dim=0)

    def _decode_and_redistribute(self, generated_ids_batch: torch.Tensor) -> List[torch.Tensor]:
        """
        å¾æ¨¡å‹ç”Ÿæˆçš„ Token åºåˆ—ä¸­æå–éŸ³è¨Š Codesï¼Œä¸¦å°‡å…¶é‡æ–°åˆ†é…å› SNAC æ‰€éœ€çš„å¤šå±¤æ ¼å¼ã€‚
        æ­¤å‡½å¼æ˜¯ `prepare_dataset.py` ä¸­ `tokenise_audio` çš„é€†å‘æ“ä½œã€‚
        """
        token_start_speech, token_end_speech = 128257, 128258
        output_waveforms = []

        for row in generated_ids_batch:
            # 1. å®šä½ <|start_of_speech|> å’Œ <|end_of_speech|>ï¼Œæå–ä¸­é–“çš„éŸ³è¨Šç¢¼
            start_indices = (row == token_start_speech).nonzero(as_tuple=True)[0]
            end_indices = (row == token_end_speech).nonzero(as_tuple=True)[0]
            
            if len(start_indices) == 0 or len(end_indices) == 0:
                output_waveforms.append(torch.tensor([]))
                continue
            
            # æå–æœ€å¾Œä¸€çµ„èªéŸ³ Tokens
            audio_codes_flat = row[start_indices[-1] + 1 : end_indices[-1]]
            
            # 2. æ¸…ç†ä¸¦ç¢ºä¿é•·åº¦æ˜¯ 7 çš„å€æ•¸ (æ¯å€‹éŸ³è¨Šå¹€ç”± 7 å€‹ Token çµ„æˆ)
            new_length = (audio_codes_flat.size(0) // 7) * 7
            trimmed_codes = audio_codes_flat[:new_length]
            final_codes = [t.item() - 128266 for t in trimmed_codes] # æ¸›å» ID åç§»é‡
            
            if not final_codes:
                output_waveforms.append(torch.tensor([]))
                continue

            # 3. å°‡æ‰å¹³åŒ–çš„ Token åºåˆ— "è§£äº¤éŒ¯" (de-interleave) å›ä¸‰å€‹å±¤æ¬¡
            layer_1, layer_2, layer_3 = [], [], []
            num_frames = len(final_codes) // 7
            for i in range(num_frames):
                base = 7 * i
                layer_1.append(final_codes[base])
                layer_2.append(final_codes[base + 1] - 4096)
                layer_3.append(final_codes[base + 2] - (2 * 4096))
                layer_3.append(final_codes[base + 3] - (3 * 4096))
                layer_2.append(final_codes[base + 4] - (4 * 4096))
                layer_3.append(final_codes[base + 5] - (5 * 4096))
                layer_3.append(final_codes[base + 6] - (6 * 4096))
            
            # 4. ä½¿ç”¨ SNAC æ¨¡å‹å°‡ä¸‰å±¤ Codes è§£ç¢¼å›éŸ³è¨Šæ³¢å½¢
            codes_for_snac = [torch.tensor(layer, dtype=torch.int32).unsqueeze(0) for layer in [layer_1, layer_2, layer_3]]
            with torch.no_grad():
                audio_hat = self.snac_model.decode(codes_for_snac)
            output_waveforms.append(audio_hat)
            
        return output_waveforms

    def synthesize(self, prompt: str, voice: str, output_path: str, speed_rate: float = 1.0):
        """
        å°å–®ä¸€æç¤ºè©é€²è¡Œç«¯åˆ°ç«¯çš„èªéŸ³åˆæˆï¼Œä¸¦å„²å­˜ç‚º WAV æª”æ¡ˆã€‚

        Args:
            prompt (str): è¦åˆæˆçš„æ–‡æœ¬ã€‚
            voice (str): ä½¿ç”¨çš„è²éŸ³è§’è‰²åç¨± (å¿…é ˆèˆ‡è¨“ç·´è³‡æ–™ä¸€è‡´)ã€‚
            output_path (str): WAV æª”æ¡ˆçš„å„²å­˜è·¯å¾‘ã€‚
            speed_rate (float, optional): èªé€Ÿèª¿æ•´æ¯”ä¾‹ã€‚>1 åŠ é€Ÿï¼Œ<1 æ¸›é€Ÿã€‚é è¨­ç‚º 1.0ã€‚
        """
        print(f"\n>>> æ­£åœ¨åˆæˆ: '{prompt}'")
        input_ids, attention_mask = self._prepare_prompts_for_batch([prompt], voice)
        input_ids, attention_mask = input_ids.to(self.model.device), attention_mask.to(self.model.device)
        
        with torch.no_grad():
            generated_ids = self.model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=1200,      # æœ€å¤§ç”Ÿæˆ Token æ•¸ï¼Œé˜²æ­¢ç„¡é™ç”Ÿæˆ
                do_sample=True,          # å•Ÿç”¨æ¡æ¨£ï¼Œå¢åŠ ç”Ÿæˆå¤šæ¨£æ€§
                temperature=0.4,         # æ§åˆ¶ç”Ÿæˆçš„éš¨æ©Ÿæ€§ï¼Œè¶Šä½è¶Šç¢ºå®š
                top_p=0.65,              # æ ¸æ¡æ¨£ï¼Œé™åˆ¶æ¡æ¨£ç¯„åœ
                repetition_penalty=1.5,  # é‡è¤‡æ‡²ç½°ï¼Œé¿å…ç”Ÿæˆé‡è¤‡è©èª
                eos_token_id=128258,     # <|end_of_speech|> ä½œç‚ºçµæŸç¬¦
            )
        
        # å°‡ç”Ÿæˆçš„ Tokens è§£ç¢¼å›éŸ³è¨Š
        output_samples = self._decode_and_redistribute(generated_ids.to("cpu"))
        
        if output_samples and output_samples[0].numel() > 0:
            audio_numpy = output_samples[0].squeeze().numpy()
            
            # æ‡‰ç”¨èªé€Ÿèª¿æ•´
            if speed_rate != 1.0:
                print(f"    >>> æ­£åœ¨èª¿æ•´èªé€Ÿç‚º: {speed_rate}x")
                audio_numpy = pyrb.time_stretch(y=audio_numpy, sr=24000, rate=speed_rate)
            
            sf.write(output_path, audio_numpy, 24000)
            print(f"âœ… éŸ³è¨Šå·²æˆåŠŸå„²å­˜è‡³: {output_path}")
        else:
            print("âš ï¸ è­¦å‘Šï¼šæœªç”Ÿæˆæœ‰æ•ˆéŸ³è¨Šã€‚")


# ==============================================================================
# æ­¥é©Ÿ 4: ä¸»ç¨‹å¼åŸ·è¡Œå€å¡Š (Main Execution Block)
# ==============================================================================
if __name__ == '__main__':
    # --- çµ„æ…‹è¨­å®š ---
    MODEL_ID = "your-hf-username/your-finetuned-model-name"
    TOKENIZER_ID = "your-hf-username/your-finetuned-model-name"
    
    # `CHOSEN_VOICE` å¿…é ˆèˆ‡æ‚¨è¨“ç·´è³‡æ–™ metadata.txt ä¸­çš„ speaker_name å®Œå…¨ä¸€è‡´ï¼
    CHOSEN_VOICE = "èªè€…1"

    # åœ¨æ­¤è™•å®šç¾©æ‚¨æƒ³ä¸€æ¬¡æ€§ç”Ÿæˆçš„å¥å­åˆ—è¡¨
    PROMPTS_TO_GENERATE = [
        "å¤§å®¶å¥½ï¼Œæ­¡è¿ä¾†åˆ°ä»Šå¤©çš„è·æ¶¯åˆ†äº«ã€‚",
        "é€™æ¬¾ç”¢å“çš„è¨­è¨ˆç†å¿µæ˜¯ç°¡ç´„èˆ‡å¯¦ç”¨æ€§çš„å®Œç¾çµåˆã€‚",
        "åœ¨å¿«é€Ÿè®ŠåŒ–çš„å¸‚å ´ä¸­ï¼Œæˆ‘å€‘å¿…é ˆä¿æŒæ•æ·ä¸¦æŒçºŒå‰µæ–°ã€‚",
        "ç¥å¤§å®¶æœ‰å€‹ç¾å¥½çš„ä¸€å¤©ï¼"
    ]

    # è¨­å®šè¼¸å‡ºè³‡æ–™å¤¾å’Œèªé€Ÿ
    OUTPUT_DIR = "./output_batch"
    SPEED_RATE = 1.0 # 1.0 ç‚ºåŸé€Ÿ, > 1.0 åŠ é€Ÿ, < 1.0 æ¸›é€Ÿ

    try:
        print("--- å•Ÿå‹•æ‰¹æ¬¡è™•ç†æ¨ç†ä»»å‹™ ---")
        # 1. åƒ…åœ¨ç¨‹å¼å•Ÿå‹•æ™‚è¼‰å…¥å’Œå„ªåŒ–æ¨¡å‹ä¸€æ¬¡
        tts_engine = OptimizedOrpheusTTS(model_id=MODEL_ID, tokenizer_id=TOKENIZER_ID)
        
        # 2. å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
        Path(OUTPUT_DIR).mkdir(exist_ok=True)
        
        # 3. éæ­·å¥å­åˆ—è¡¨ä¸¦é€ä¸€ç”ŸæˆéŸ³è¨Š
        for i, prompt in enumerate(PROMPTS_TO_GENERATE):
            # ç‚ºæ¯å€‹æª”æ¡ˆç”¢ç”Ÿä¸€å€‹å”¯ä¸€çš„åç¨±
            output_filename = f"{OUTPUT_DIR}/batch_output_{CHOSEN_VOICE}_{i+1:03d}.wav"
            tts_engine.synthesize(prompt, CHOSEN_VOICE, output_filename, speed_rate=SPEED_RATE)

        print("\n--- âœ… æ‰€æœ‰æ‰¹æ¬¡ä»»å‹™å·²å®Œæˆ ---")

    except Exception as e:
        import traceback
        print("\nâŒ ç¨‹å¼åŸ·è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼")
        traceback.print_exc()